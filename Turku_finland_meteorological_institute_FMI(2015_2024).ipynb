{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "turku_finland_meteorological_institute_FMI(2015-2024).ipynb",
      "authorship_tag": "ABX9TyP5Q713yeI2fhY/lBx7ggKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Climatic_Data/blob/main/Turku_finland_meteorological_institute_FMI(2015_2024).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlyr4KCnLd0",
        "outputId": "8a12470f-9b17-499a-daae-c5452afe774c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fetching Year 2015 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2016 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2017 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2018 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2019 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2020 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2021 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2022 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2023 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "--- Fetching Year 2024 ---\n",
            "  Progress: 0/53 chunks\n",
            "  Progress: 15/53 chunks\n",
            "  Progress: 30/53 chunks\n",
            "  Progress: 45/53 chunks\n",
            "\n",
            "Success! File saved as Turku_Artukainen_10Yr_Averages.csv\n",
            "Final dataset contains 8760 rows (8760 = full year).\n",
            "Type  Month  Day  Hour  Temperature_C  Wind_Speed_ms\n",
            "0         1    1     0      -0.694286       2.788889\n",
            "1         1    1     1      -0.878333       2.807407\n",
            "2         1    1     2      -1.171667       3.183333\n",
            "3         1    1     3      -1.503333       3.031481\n",
            "4         1    1     4      -1.736667       3.064815\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "from datetime import datetime, timedelta, UTC\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "STATION_ID = \"100949\"  # Turku Artukainen\n",
        "YEARS = range(2015, 2025)  # 2015 to 2024 inclusive\n",
        "OUTPUT_FILE = \"Turku_Artukainen_10Yr_Averages.csv\"\n",
        "\n",
        "# Simplified tasks: No Solar\n",
        "tasks = [\n",
        "    {\n",
        "        \"name\": \"Weather\",\n",
        "        \"query\": \"fmi::observations::weather::simple\",\n",
        "        \"params\": \"t2m,ws_10min\",\n",
        "        \"map\": {\"t2m\": \"Temperature_C\", \"ws_10min\": \"Wind_Speed_ms\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "def get_chunks(start_date, end_date):\n",
        "    s = datetime.strptime(start_date, \"%Y-%m-%d\").replace(tzinfo=UTC)\n",
        "    e = datetime.strptime(end_date, \"%Y-%m-%d\").replace(tzinfo=UTC)\n",
        "    chunks = []\n",
        "    curr = s\n",
        "    while curr < e:\n",
        "        nxt = min(curr + timedelta(days=7), e)\n",
        "        chunks.append((curr.strftime('%Y-%m-%dT%H:%M:%SZ'), nxt.strftime('%Y-%m-%dT%H:%M:%SZ')))\n",
        "        curr = nxt\n",
        "    return chunks\n",
        "\n",
        "all_years_data = []\n",
        "\n",
        "# --- FETCHING LOOP ---\n",
        "for year in YEARS:\n",
        "    print(f\"\\n--- Fetching Year {year} ---\")\n",
        "    start_str = f\"{year}-01-01\"\n",
        "    end_str = f\"{year+1}-01-01\"\n",
        "    chunks = get_chunks(start_str, end_str)\n",
        "\n",
        "    all_rows = []\n",
        "    for i, (start_ch, end_ch) in enumerate(chunks):\n",
        "        if i % 15 == 0: print(f\"  Progress: {i}/{len(chunks)} chunks\")\n",
        "\n",
        "        params = {\n",
        "            \"service\": \"WFS\", \"version\": \"2.0.0\", \"request\": \"getFeature\",\n",
        "            \"storedquery_id\": tasks[0]['query'],\n",
        "            \"fmisid\": STATION_ID,\n",
        "            \"parameters\": tasks[0]['params'],\n",
        "            \"starttime\": start_ch, \"endtime\": end_ch\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            r = requests.get(\"http://opendata.fmi.fi/wfs\", params=params, timeout=30)\n",
        "            if r.status_code == 200:\n",
        "                root = ET.fromstring(r.content)\n",
        "                ns = {'wfs': 'http://www.opengis.net/wfs/2.0', 'BsWfs': 'http://xml.fmi.fi/schema/wfs/2.0'}\n",
        "\n",
        "                for member in root.findall('.//wfs:member', ns):\n",
        "                    elm = member.find('.//BsWfs:BsWfsElement', ns)\n",
        "                    if elm is not None:\n",
        "                        t = elm.find('BsWfs:Time', ns).text\n",
        "                        p = elm.find('BsWfs:ParameterName', ns).text\n",
        "                        v_node = elm.find('BsWfs:ParameterValue', ns)\n",
        "                        try:\n",
        "                            v = float(v_node.text) if v_node is not None and v_node.text != 'NaN' else None\n",
        "                            if v is not None:\n",
        "                                all_rows.append({'Time': t, 'Type': tasks[0]['map'][p], 'Value': v})\n",
        "                        except: continue\n",
        "        except Exception as e:\n",
        "            print(f\"  Error in chunk: {e}\")\n",
        "        time.sleep(0.05)\n",
        "\n",
        "    if all_rows:\n",
        "        df_year = pd.DataFrame(all_rows)\n",
        "        df_year['Time'] = pd.to_datetime(df_year['Time'])\n",
        "        df_year = df_year.pivot_table(index='Time', columns='Type', values='Value')\n",
        "        # Fill minor gaps up to 1 hour\n",
        "        df_year = df_year.interpolate(method='linear', limit=6)\n",
        "        all_years_data.append(df_year.reset_index())\n",
        "\n",
        "# --- PROCESSING AVERAGES ---\n",
        "if all_years_data:\n",
        "    master_df = pd.concat(all_years_data)\n",
        "\n",
        "    # 1. Remove Leap Year (Feb 29)\n",
        "    master_df = master_df[~((master_df['Time'].dt.month == 2) & (master_df['Time'].dt.day == 29))]\n",
        "\n",
        "    # 2. Add Keys\n",
        "    master_df['Month'] = master_df['Time'].dt.month\n",
        "    master_df['Day'] = master_df['Time'].dt.day\n",
        "    master_df['Hour'] = master_df['Time'].dt.hour\n",
        "\n",
        "    # 3. Group by Month, Day, Hour to get the 10-year Average\n",
        "    climate_avg = master_df.groupby(['Month', 'Day', 'Hour']).mean(numeric_only=True).reset_index()\n",
        "\n",
        "    # 4. Final Sort and Save\n",
        "    climate_avg = climate_avg.sort_values(['Month', 'Day', 'Hour'])\n",
        "    climate_avg.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\nSuccess! File saved as {OUTPUT_FILE}\")\n",
        "    print(f\"Final dataset contains {len(climate_avg)} rows (8760 = full year).\")\n",
        "    print(climate_avg.head())\n",
        "else:\n",
        "    print(\"No data retrieved.\")"
      ]
    }
  ]
}