{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGS3Ae6FOTkACcK1kYOugI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/General/blob/main/Turku_ERA5_CDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"cdsapi>=0.7.7\""
      ],
      "metadata": {
        "id": "ejepqwjFn8Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netcdf4"
      ],
      "metadata": {
        "id": "RvPUR0yStJcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Updated configuration: removed the UID prefix as per the error instructions\n",
        "content = \"\"\"url: https://cds.climate.copernicus.eu/api\n",
        "key: c025f203-5930-4d9c-acd6-699c46be7fd8\"\"\"\n",
        "\n",
        "with open(os.path.expanduser('~/.cdsapirc'), 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"Configuration updated! Now attempting to update the library...\")\n",
        "\n",
        "# Also update your library to the latest version to match the new API\n",
        "!pip install --upgrade cdsapi"
      ],
      "metadata": {
        "id": "xsU63P_1qIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cdsapi\n",
        "import os\n",
        "import calendar\n",
        "\n",
        "# CRITICAL FIX: Set progress=False to prevent widget metadata creation\n",
        "c = cdsapi.Client(progress=False)\n",
        "\n",
        "# Range: Full year 2024 + January 2025\n",
        "tasks = [('2024', str(m).zfill(2)) for m in range(1, 13)] + [('2025', '01')]\n",
        "\n",
        "print(\"--- Data Download Started (GitHub-Safe Mode) ---\")\n",
        "\n",
        "for i, (year, month) in enumerate(tasks):\n",
        "    filename = f'turku_{year}_{month}.nc'\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"[{i+1}/13] {filename} already exists. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Determine days: Full month for 2024, but only Jan 1st for 2025\n",
        "    if year == '2025':\n",
        "        days = ['01']\n",
        "    else:\n",
        "        last_day = calendar.monthrange(int(year), int(month))[1]\n",
        "        days = [str(d).zfill(2) for d in range(1, last_day + 1)]\n",
        "\n",
        "    print(f\"[{i+1}/13] Requesting {month}/{year}...\")\n",
        "\n",
        "    try:\n",
        "        c.retrieve(\n",
        "            'reanalysis-era5-land',\n",
        "            {\n",
        "                'variable': [\n",
        "                    '2m_temperature',\n",
        "                    '10m_u_component_of_wind',\n",
        "                    '10m_v_component_of_wind',\n",
        "                    'surface_solar_radiation_downwards',\n",
        "                ],\n",
        "                'year': year,\n",
        "                'month': month,\n",
        "                'day': days,\n",
        "                'time': [f\"{str(h).zfill(2)}:00\" for h in range(24)],\n",
        "                'area': [60.5, 22.5, 60.5, 22.5],\n",
        "                'format': 'netcdf',\n",
        "            },\n",
        "            filename)\n",
        "        # We print a success message that will stay in your GitHub logs\n",
        "        print(f\"      ‚úÖ {filename} successfully downloaded and saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ùå Failed {year}-{month}: {e}\")\n",
        "\n",
        "print(\"--- All Downloads Complete ---\")"
      ],
      "metadata": {
        "id": "FMNRWkaGoHXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netcdf4 h5netcdf"
      ],
      "metadata": {
        "id": "BEbXSK_zQsp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 1. Setup paths\n",
        "file_list = sorted(glob.glob('turku_*.nc'))\n",
        "extract_dir = 'extracted_temp'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "all_data = []\n",
        "\n",
        "print(f\"Checking {len(file_list)} files for ZIP compression...\")\n",
        "\n",
        "for f in file_list:\n",
        "    try:\n",
        "        # Check if the file is actually a ZIP (first two bytes are 'PK')\n",
        "        with open(f, 'rb') as test_f:\n",
        "            is_zip = test_f.read(2) == b'PK'\n",
        "\n",
        "        if is_zip:\n",
        "            print(f\"üì¶ {f} is a ZIP archive. Extracting...\")\n",
        "            with zipfile.ZipFile(f, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "\n",
        "            # Find the actual .nc file that was inside the ZIP\n",
        "            inner_files = glob.glob(os.path.join(extract_dir, \"*.nc\"))\n",
        "            if inner_files:\n",
        "                # Process the extracted file\n",
        "                ds = xr.open_dataset(inner_files[0], engine='netcdf4')\n",
        "                df = ds.to_dataframe().reset_index()\n",
        "                all_data.append(df)\n",
        "                print(f\"‚úÖ Successfully read data from inside {f}\")\n",
        "\n",
        "                # Clean up extracted file to save space\n",
        "                for extra in inner_files:\n",
        "                    os.remove(extra)\n",
        "        else:\n",
        "            # It's a normal NetCDF file\n",
        "            ds = xr.open_dataset(f, engine='netcdf4')\n",
        "            df = ds.to_dataframe().reset_index()\n",
        "            all_data.append(df)\n",
        "            print(f\"‚úÖ Successfully read normal NetCDF: {f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {f}: {e}\")\n",
        "\n",
        "# 2. Final Merge and Conversion\n",
        "if all_data:\n",
        "    df_combined = pd.concat(all_data).sort_values('valid_time')\n",
        "\n",
        "    # Conversions\n",
        "    df_combined['Air_Temp_C'] = df_combined['t2m'] - 273.15\n",
        "    df_combined['Wind_Speed_ms'] = np.sqrt(df_combined['u10']**2 + df_combined['v10']**2)\n",
        "    df_combined['GHI_Wm2'] = df_combined['ssrd'] / 3600\n",
        "\n",
        "    # Clean up and Export\n",
        "    final_output = df_combined[['valid_time', 'Air_Temp_C', 'Wind_Speed_ms', 'GHI_Wm2']]\n",
        "    final_output.columns = ['Timestamp', 'Air_Temp_C', 'Wind_Speed_ms', 'GHI_W_m2']\n",
        "\n",
        "    final_output.to_csv('Turku_Final_Weather_2024.csv', index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"SUCCESS! Final CSV created.\")\n",
        "    print(f\"Total rows: {len(final_output)}\")\n",
        "    print(\"=\"*30)\n",
        "    print(final_output.head())\n",
        "else:\n",
        "    print(\"üõë No data frames were created. Check if files are 0 KB.\")"
      ],
      "metadata": {
        "id": "eyrOqHk1sSjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "432p5jDJSAqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1. Get the name of your current notebook\n",
        "# If you are in Colab, it is usually 'Untitled.ipynb' unless you renamed it\n",
        "notebook_path = 'your_notebook_name.ipynb'\n",
        "\n",
        "try:\n",
        "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "        nb_data = json.load(f)\n",
        "\n",
        "    # 2. Check if the 'widgets' key exists in metadata and remove it\n",
        "    if 'widgets' in nb_data.get('metadata', {}):\n",
        "        del nb_data['metadata']['widgets']\n",
        "        print(\"‚úÖ Success: Broken widget metadata removed.\")\n",
        "\n",
        "        # 3. Save the notebook back to disk\n",
        "        with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(nb_data, f, indent=1)\n",
        "        print(\"‚ú® Notebook is now GitHub-safe. You can now push/upload it.\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No widget metadata found. The error might be in the file name or a specific cell's output.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Could not find '{notebook_path}'. Please check the filename in the sidebar.\")"
      ],
      "metadata": {
        "id": "Q3zBt6pWRZ9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}